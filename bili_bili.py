# _*_ coding : utf-8 _*_
# @Time : 2024/10/28 0:15
# @Author : å“¥å‡ ä¸ªä½›
# @File : bili_bili
# @Project : CrawlerJDComment
import json
import os
import queue
import re
import subprocess
import sys
import time
import uuid

import requests
from lxml import etree
from tqdm import tqdm

from utils.custom_logger import logger
from utils.thread_pool import start_thread_pool_and_executor_tasks

# å®šä¹‰éæ³•å­—ç¬¦åŠå…¶æ›¿æ¢å­—ç¬¦
illegal_chars = {
    '<': '_',
    '>': '_',
    ':': '_',
    '"': '_',
    '/': '_',
    '\\': '_',
    '|': '_',
    '?': '_',
    '*': '_'
}

FFMPEG_PATH = r"E:\AllStudyExe\Python\ffmpeg-2024-10-27\bin\ffmpeg.exe"
FFPROBE_PATH = r"E:\AllStudyExe\Python\ffmpeg-2024-10-27\bin\ffprobe.exe"


def get_video_info(vide_file_path):
    """ä½¿ç”¨ ffprobe è·å–è§†é¢‘ä¿¡æ¯"""
    command = [
        FFPROBE_PATH,
        '-v', 'quiet',  # é™é»˜æ¨¡å¼
        '-print_format', 'json',  # è¾“å‡ºæ ¼å¼ä¸º JSON
        '-show_format',  # æ˜¾ç¤ºæ ¼å¼ä¿¡æ¯
        '-show_streams',  # æ˜¾ç¤ºæµä¿¡æ¯
        vide_file_path
    ]

    try:
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8',
                                errors='replace')
        result.check_returncode()  # æ£€æŸ¥å‘½ä»¤æ˜¯å¦æˆåŠŸæ‰§è¡Œ
        video_info = json.loads(result.stdout)
        return video_info
    except subprocess.CalledProcessError as e:
        logger.error(f"è°ƒç”¨ ffprobe æ—¶å‡ºé”™: {e}")
        logger.error(f"å‘½ä»¤: {' '.join(command)}")
        logger.error(f"è¿”å›ç : {e.returncode}")
        logger.error(f"æ ‡å‡†é”™è¯¯è¾“å‡º: {e.stderr}")
        return None
    except json.JSONDecodeError as e:
        logger.error(f"è§£æ JSON æ—¶å‡ºé”™: {e}")
        return None


def parse_video_info(video_info):
    """è§£æè§†é¢‘ä¿¡æ¯"""
    if not video_info:
        return

    format_info = video_info.get('format', {})
    streams = video_info.get('streams', [])

    # è§†é¢‘æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯
    logger.info("è§†é¢‘æ–‡ä»¶åŸºæœ¬ä¿¡æ¯:")
    logger.info(f"æ–‡ä»¶å: {format_info.get('filename')}")
    logger.info(f"æ ¼å¼: {format_info.get('format_name')}")
    logger.info(f"æŒç»­æ—¶é—´: {format_info.get('duration')} ç§’")
    logger.info(f"æ€»æ¯”ç‰¹ç‡: {int(format_info.get('bit_rate')) // 1000} kbps")

    # è§†é¢‘æµä¿¡æ¯
    for stream in streams:
        if stream['codec_type'] == 'video':
            logger.info("\nè§†é¢‘æµä¿¡æ¯:")
            logger.info(f"ç¼–è§£ç å™¨: {stream.get('codec_name')}")
            logger.info(f"åˆ†è¾¨ç‡: {stream.get('width')}x{stream.get('height')}")
            logger.info(f"æ¯”ç‰¹ç‡: {int(stream.get('bit_rate')) // 1000} kbps")
            logger.info(f"å¸§ç‡: {stream.get('r_frame_rate')}")
            logger.info(f"ç¼–ç æ ¼å¼: {stream.get('codec_tag_string')}")

        if stream['codec_type'] == 'audio':
            logger.info("\néŸ³é¢‘æµä¿¡æ¯:")
            logger.info(f"ç¼–è§£ç å™¨: {stream.get('codec_name')}")
            logger.info(f"é‡‡æ ·ç‡: {stream.get('sample_rate')} Hz")
            logger.info(f"æ¯”ç‰¹ç‡: {int(stream.get('bit_rate')) // 1000} kbps")
            logger.info(f"é€šé“æ•°: {stream.get('channels')}")


def merge_files(log_path, video_file_path, audio_file_path, out_file_path, resolution=(1920, 1080), bitrate=6000000):
    """åˆå¹¶éŸ³è§†é¢‘å¹¶è°ƒæ•´è§†é¢‘æ¸…æ™°åº¦"""
    start_time = time.time()
    logger.info(f"å¼€å§‹åˆå¹¶éŸ³è§†é¢‘ï¼Œåˆ†è¾¨ç‡: {resolution}ï¼Œæ¯”ç‰¹ç‡: {bitrate}")

    ffmpeg_command = [
        FFMPEG_PATH,
        '-y',  # æ·»åŠ æ­¤å‚æ•°ï¼Œç”¨äºåœ¨è§†é¢‘æ–‡ä»¶å·²å­˜åœ¨æ—¶è¦†ç›–å®ƒ
        '-i', video_file_path,
        '-i', audio_file_path,
        '-vf', 'scale={}:{}'.format(*resolution),
        '-b:v', '{}'.format(bitrate),
        '-c:v', 'libx264',
        '-c:a', 'aac',
        out_file_path
    ]

    # å¯åŠ¨FFmpegè¿›ç¨‹
    process = subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True,
                               encoding='utf-8')

    # è¯»å–FFmpegçš„è¾“å‡ºå¹¶è§£æè¿›åº¦ä¿¡æ¯
    progress_pattern = re.compile(r'time=(\d{2}:\d{2}:\d{2}\.\d{2})')
    total_duration = None
    total_seconds = None

    # ä»FFmpegè¾“å‡ºä¸­æå–æ€»æ—¶é•¿
    for line in process.stdout:
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(line)
            f.flush()  # ç«‹å³å†™å…¥ç£ç›˜
        if 'Duration' in line:
            duration_match = re.search(r'Duration: (\d{2}:\d{2}:\d{2}\.\d{2})', line)
            if duration_match:
                total_duration = duration_match.group(1)
                total_seconds = sum(float(x) * 60 ** i for i, x in enumerate(reversed(total_duration.split(':'))))
                break

    # å¦‚æœæ‰¾åˆ°äº†æ€»æ—¶é•¿ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
    if total_duration:
        with tqdm(total=total_seconds, unit='s', desc=f"Processing {resolution} {bitrate}") as pbar:
            for line in process.stdout:
                match = progress_pattern.search(line)
                if match:
                    current_time = match.group(1)
                    current_seconds = sum(
                        float(x) * 60 ** i for i, x in enumerate(reversed(current_time.split(':'))))
                    pbar.update(current_seconds - pbar.n)
                    pbar.set_description(f"Processing {resolution} {bitrate}: {current_time} / {total_duration}")
                    logger.info(f"Progress {resolution} {bitrate}: {current_time} / {total_duration}")

    # ç­‰å¾…FFmpegè¿›ç¨‹ç»“æŸ
    process.wait()

    end_time = time.time()
    execution_time = end_time - start_time

    if process.returncode != 0:
        logger.error(f"FFmpeg process for {resolution} {bitrate} failed with return code {process.returncode}")
    else:
        logger.info(f"åˆå¹¶éŸ³è§†é¢‘å®Œæˆï¼Œåˆ†è¾¨ç‡: {resolution}ï¼Œæ¯”ç‰¹ç‡: {bitrate}ï¼Œåˆæˆæ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")


def download_file(url, file_path, headers):
    """ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦æ¡"""
    try:
        response = requests.get(url, headers=headers, stream=True)
        response.raise_for_status()

        # è·å–æ–‡ä»¶å¤§å°
        total_size = int(response.headers.get('content-length', 0))

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # åˆå§‹åŒ–è¿›åº¦æ¡
        with tqdm(total=total_size, unit='B', unit_scale=True,
                  desc=f"Downloading {os.path.basename(file_path)}") as pbar:
            with open(file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:  # è¿‡æ»¤æ‰ä¿æŒæ´»åŠ¨çš„ç©ºchunk
                        f.write(chunk)
                        pbar.update(len(chunk))
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False
    return True


def download_video_and_audio(video_url, audio_url, video_file_path, audio_file_path, headers):
    """ä¸‹è½½è§†é¢‘å’ŒéŸ³é¢‘æ–‡ä»¶"""
    logger.info("å¼€å§‹ä¸‹è½½è§†é¢‘å’ŒéŸ³é¢‘")

    if not download_file(video_url, video_file_path, headers):
        return False
    if not download_file(audio_url, audio_file_path, headers):
        return False

    logger.info("ä¸‹è½½è§†é¢‘å’ŒéŸ³é¢‘å®Œæˆ")
    return True


def select_video_stream(info_dict, target_quality=80):
    """é€‰æ‹©åˆé€‚çš„è§†é¢‘æµ"""
    video_streams = None
    try:
        """çŸ­è§†é¢‘"""
        video_streams = info_dict["data"]["dash"]['video']
    except Exception:
        try:
            """ç”µè§†å‰§"""
            video_streams = info_dict["result"]["video_info"]['dash']['video']
        except Exception:
            logger.error(f"è§†é¢‘æµä¸ºNoneï¼š{video_streams}")
    for stream in video_streams:
        if stream['id'] == target_quality:
            return stream
    logger.warning(f"æ²¡æœ‰æ‰¾åˆ°è´¨é‡ä¸º {target_quality} çš„è§†é¢‘æµï¼Œé€‰æ‹©é»˜è®¤çš„æœ€é«˜è´¨é‡æµ")
    return video_streams[0]


def main(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/130.0.0.0 Safari/537.36',
        'Referer': url,
        'Cookie': "buvid3=1C1BC5B2-006E-1A59-5713-886DA7A228EB42937infoc; b_nut=1712835742; "
                  "_uuid=8D1022CF1-6AED-AC76-1F8A-6B5A1107105610B43285infoc; "
                  "buvid4=468FA1FA-CBB4-5C9F-7F54-FEE3EABEA88443901-024041111-QNkT6TeeovUOz7Mw4HO0hw%3D%3D; "
                  "DedeUserID=213793137; DedeUserID__ckMd5=9b15a941ec0fce66; CURRENT_BLACKGAP=0; rpdid=|("
                  "k)~lmllmuu0J'u~uk)Jl~~Y; enable_web_push=ENABLE; iflogin_when_web_push=1; "
                  "header_theme_version=CLOSE; FEED_LIVE_VERSION=V_WATCHLATER_PIP_WINDOW3; buvid_fp_plain=undefined; "
                  "LIVE_BUVID=AUTO4617154310584985; home_feed_column=5; is-2022-channel=1; "
                  "fingerprint=500cd5faf91ce13e34cd2eec02e56a47; "
                  "SESSDATA=b9609e2b%2C1743670368%2C8667a%2Aa1CjD-L1nP279Je-bAPRzQ-2P_HBj5pFHzyvu5E"
                  "-JaVU2RnWZvTSTvwiZHsijfrZXFbUsSVmVCbmt3dExXaXQ0dmwwMk1kOXRWSzFEZzgwX01FbjNQOE5LWldFak9Td0gwUWt2d19nR1JnTW01S1F1aXdrRERUdmJlVGJleWpJbkdIZ0Q0Q0JQUG9nIIEC; bili_jct=98c05b382e6e2852e57ae41190859c9e; PVID=1; buvid_fp=500cd5faf91ce13e34cd2eec02e56a47; hit-dyn-v2=1; CURRENT_FNVAL=4048; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MzAyMjQyODEsImlhdCI6MTcyOTk2NTAyMSwicGx0IjotMX0.hj5YKXRA7EHj8AO_l4MpwP9QeoQoZqmMrGBeLK9eSLE; bili_ticket_expires=1730224221; sid=5ebbo7ef; browser_resolution=1550-745; b_lsid=FD810A910D_192D3DB0FF1; bp_t_offset_213793137=993397709039730688; CURRENT_QUALITY=80"
    }

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"è¯·æ±‚è§†é¢‘é¡µé¢æ—¶å‡ºé”™: {e}")
        return

    res_text = response.text
    print(res_text)

    tree = etree.HTML(res_text)

    try:
        """çŸ­è§†é¢‘"""
        base_info = "".join(tree.xpath("/html/head/script[4]/text()"))[20:]
        print(base_info)
        info_dict = json.loads(base_info)

        # æ ‡é¢˜
        title_json = "".join(tree.xpath("/html/head/script[5]/text()"))[25:]
        print(title_json)
        # å»æ‰å¤šä½™çš„å­—ç¬¦ï¼Œåªä¿ç•™ JSON éƒ¨åˆ†
        json_str = title_json.split(';(function')[0]
        print(json_str)
        data = json.loads(json_str)
        title = data["videoData"]["title"]
    except Exception:
        try:
            """ç”µè§†å‰§"""
            base_info = "".join(tree.xpath("/html/head/script[4]/text()"))
            print(base_info)
            match = re.search(r'const\s+playurlSSRData\s*=\s*({[^;]*}}}})', base_info, re.DOTALL | re.MULTILINE)
            info_dict = json.loads(match.group(1))
            print(info_dict)

            # æ ‡é¢˜
            title = info_dict["result"]["play_view_business_info"]["episode_info"]["long_title"]
        except Exception:
            logger.error(f"æ²¡æœ‰è¯¥è§†é¢‘ç±»å‹ï¼")
            return

    # æ›¿æ¢éæ³•å­—ç¬¦
    title = ''.join(illegal_chars.get(char, char) for char in title)
    title = f"{title}-{uuid.uuid4()}"
    html_path = fr"static\b\{title}.html"  # htmlè·¯å¾„
    log_path = fr'static\b\{title}_ffmpeg_output.log'  # æ—¥å¿—è·¯å¾„
    video_file_path = fr"static\b\{title}_video.mp4"  # è§†é¢‘è·¯å¾„
    audio_file_path = fr"static\b\{title}_audio.mp4"  # éŸ³é¢‘è·¯å¾„
    out_file_path = fr"static\b\{title}.mp4"  # éŸ³è§†é¢‘åˆæˆè·¯å¾„

    # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå­˜åœ¨åˆ™ä½¿ç”¨uuidå†å­˜ï¼ˆæ‰€æœ‰ï¼‰
    rand_uuid = str(uuid.uuid4()).replace("-", "")
    file_exists_list = [html_path, log_path, video_file_path, audio_file_path, out_file_path]
    for i, file_exists in enumerate(file_exists_list):
        if os.path.exists(file_exists):
            file_name, file_extension = os.path.splitext(file_exists)
            file_exists_list[i] = f"{file_name}_{rand_uuid}{file_extension}"
            sys.exit('ğŸ˜­')

    # é‡æ–°èµ‹å€¼
    html_path = file_exists_list[0]
    log_path = file_exists_list[1]
    video_file_path = file_exists_list[2]
    audio_file_path = file_exists_list[3]
    out_file_path = file_exists_list[4]

    # è®°å½•html
    try:
        with open(html_path, "w", encoding="utf8") as f:
            f.write(res_text)
    except FileNotFoundError:
        directory = os.path.dirname(html_path)
        os.makedirs(directory)
        with open(html_path, "w", encoding="utf8") as f:
            f.write(res_text)

    # é€‰æ‹©åˆé€‚çš„è§†é¢‘æµ
    target_quality = 80  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
    selected_video_stream = select_video_stream(info_dict, target_quality)
    video_url = selected_video_stream["baseUrl"]
    try:
        """çŸ­è§†é¢‘"""
        audio_url = info_dict["data"]['dash']['audio'][0]["baseUrl"]
    except Exception:
        try:
            """ç”µè§†å‰§"""
            audio_url = info_dict["result"]["video_info"]['dash']['audio'][0]["baseUrl"]
        except Exception:
            logger.error(f"æ²¡æœ‰è¯¥éŸ³é¢‘urlï¼")
            return

    # ä¸‹è½½è§†é¢‘å’ŒéŸ³é¢‘
    if not download_video_and_audio(video_url, audio_url, video_file_path, audio_file_path, headers):
        return

    # è®¡ç®—æ€»æ¯”ç‰¹ç‡
    video_bitrate = selected_video_stream["bandwidth"]  # // 1000  # å•ä½è½¬æ¢ä¸º kbps
    print("è§†é¢‘ç ç‡ï¼š", video_bitrate)
    try:
        """çŸ­è§†é¢‘"""
        audio_bitrate = info_dict["data"]['dash']['audio'][0]["bandwidth"]
    except Exception:
        try:
            """ç”µè§†å‰§"""
            audio_bitrate = info_dict["result"]["video_info"]['dash']['audio'][0]["bandwidth"]  # // 1000  # å•ä½è½¬æ¢ä¸º kbps
        except Exception:
            logger.error(f"æ²¡æœ‰è¯¥éŸ³é¢‘ç ç‡ï¼")
            return
    print("éŸ³é¢‘ç ç‡ï¼š", audio_bitrate)
    total_bitrate = video_bitrate + audio_bitrate
    print("æ€»ç ç‡ï¼š", total_bitrate)

    # è·å–åˆ†è¾¨ç‡
    width = selected_video_stream.get('width', 1920)
    height = selected_video_stream.get('height', 1080)

    # åˆå¹¶è§†é¢‘å’ŒéŸ³é¢‘
    merge_files(log_path, video_file_path, audio_file_path, out_file_path, resolution=(width, height),
                bitrate=total_bitrate)


if __name__ == '__main__':
    urls = queue.Queue()
    urls.put(
            f'https://www.bilibili.com/bangumi/play/ep456213?spm_id_from=333.337.0.0&from_spmid=666.25.episode.0')

    # å¼€å¯çº¿ç¨‹æ± å¹¶æäº¤ä»»åŠ¡
    start_thread_pool_and_executor_tasks(task_queue=urls, target_method=main)
